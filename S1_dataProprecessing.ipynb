{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 数据预处理\n",
    "\n",
    "数据分析/数据挖掘的好与坏，数据预处理起到了非常关键的作用。\n",
    "数据预处理一共有如下几个部分：\n",
    "\n",
    "1. 检查重复值、缺失值、检查离群值（异常数据）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. 检查重复值、缺失值、检查离群值（异常数据）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"dataProprecessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (1, 144.5, 5.9, 33, 'M'),\n",
    "        (2, 167.2, 5.4, 45, 'M'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (4, 144.5, 5.9, 33, 'M'),\n",
    "        (5, 133.2, 5.7, 54, 'F'),\n",
    "        (3, 124.1, 5.2, 23, 'F'),\n",
    "        (5, 129.2, 5.3, 42, 'M'),\n",
    "        ], ['id', 'weight', 'height', 'age', 'gender'])"
   ]
  },
  {
   "source": [
    "### 1.1 检查重复值\n",
    "\n",
    "先去完全相同记录，再去除id之外的重复记录，最后检查是否有id重复，如有则对数据集重新搞个id\n",
    "\n",
    "第一步，去除完全相同的数据记录\n",
    "\n",
    "方法：查看完整数据集数量和运行.distinct()方法后的数据集数量。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Count of rows: 7\nCount of distinct rows: 6\n"
    }
   ],
   "source": [
    "print('Count of rows: {0}'.format(df.count()))\n",
    "print('Count of distinct rows: {0}'.format(df.distinct().count()))"
   ]
  },
  {
   "source": [
    "这两个数字不同，说明有完全相同的数据，需要使用下面的方法去重。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  4| 144.5|   5.9| 33|     M|\n|  1| 144.5|   5.9| 33|     M|\n|  5| 129.2|   5.3| 42|     M|\n|  5| 133.2|   5.7| 54|     F|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  },
  {
   "source": [
    "第二步，去除除了ID之外完全相同的数据记录\n",
    "\n",
    "方法：只对id之外的数据使用.distinct(...)方法, 重复上面的方法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Count of ids: 6\nCount of distinct ids: 5\n"
    }
   ],
   "source": [
    "print('Count of ids: {0}'.format(df.count()))\n",
    "print('Count of distinct ids: {0}'.format(df.select(\n",
    "    [c for c in df.columns if c != 'id']\n",
    ").distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+\n| id|weight|height|age|gender|\n+---+------+------+---+------+\n|  5| 133.2|   5.7| 54|     F|\n|  4| 144.5|   5.9| 33|     M|\n|  2| 167.2|   5.4| 45|     M|\n|  3| 124.1|   5.2| 23|     F|\n|  5| 129.2|   5.3| 42|     M|\n+---+------+------+---+------+\n\n"
    }
   ],
   "source": [
    "# 两个数字不同，说明除了id之外，有重复数据\n",
    "# .dropDuplicates(), 使用subset参数来指定要处理的列\n",
    "df = df.dropDuplicates(subset=[c for c in df.columns if c != 'id'])\n",
    "df.show()"
   ]
  },
  {
   "source": [
    "第三步，检查是否有重复的id\n",
    "\n",
    "方法：使用.agg(...)对列执行指定的函数方法。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+--------+\n|count|distinct|\n+-----+--------+\n|    5|       4|\n+-----+--------+\n\n"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fun\n",
    "\n",
    "df.agg(\n",
    "    # .alias('name'), 允许给返回列取一个别名\n",
    "    fun.count('id').alias('count'),\n",
    "    fun.countDistinct('id').alias('distinct')\n",
    ").show()"
   ]
  },
  {
   "source": [
    "这两个数字不同，说明有重复的id，但是这个时候已经没有除了id列之外的数据重复，所以，我们认为id的重复是系统造成的。\n",
    "\n",
    "方法：重新给每行记录一个唯一的id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+------+-------------+\n| id|weight|height|age|gender|       new_id|\n+---+------+------+---+------+-------------+\n|  5| 133.2|   5.7| 54|     F|  25769803776|\n|  4| 144.5|   5.9| 33|     M| 171798691840|\n|  2| 167.2|   5.4| 45|     M| 592705486848|\n|  3| 124.1|   5.2| 23|     F|1236950581248|\n|  5| 129.2|   5.3| 42|     M|1365799600128|\n+---+------+------+---+------+-------------+\n\n"
    }
   ],
   "source": [
    "df.withColumn('new_id', fun.monotonically_increasing_id()).show()"
   ]
  },
  {
   "source": [
    "### 1.2检查缺失值\n",
    "\n",
    "缺失值的处理就两种方法：移除和填充\n",
    "\n",
    "(1)如果数据足够多的话，处理缺失值最简单的方法就是移除。\n",
    "\n",
    "最粗略的指标，如果移除的数据超过了原始数据集的50%的话，就需要慎重。\n",
    "\n",
    "(2)另一种方法：填充。\n",
    "   如果是布尔变量，可以添加第三类别--Missing，将其转换为一个分类变量；\n",
    "\n",
    "   如果数据是分类变量的，可以简单地扩展界别的数量，同时添加Missing类别；\n",
    "\n",
    "   如果数据是数值变量，可以通过填充平均数、中位数或者四分位数等，可以根据数据分布的形状而定。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = spark.createDataFrame([\n",
    "        (1, 143.5, 5.6, 28,   'M',  100000),\n",
    "        (2, 167.2, 5.4, 45,   'M',  None),\n",
    "        (3, None , 5.2, None, None, None),\n",
    "        (4, 144.5, 5.9, 33,   'M',  None),\n",
    "        (5, 133.2, 5.7, 54,   'F',  None),\n",
    "        (6, 124.1, 5.2, None, 'F',  None),\n",
    "        (7, 129.2, 5.3, 42,   'M',  76000),\n",
    "    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])"
   ]
  },
  {
   "source": [
    "第一步，观测每个样本（每行）以及每列的缺失值数量情况\n",
    "\n",
    "类比于spark的DataFrame的.agg(...)方法作用于列学习。作用于行的需要RDD的.map(lambda...)\n",
    "\n",
    "我们要综合行和列的缺失值情况来决定是移除还是填充缺失值。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 统计每行缺失值情况\n",
    "df_miss.rdd.map(\n",
    "    lambda row: (row['id'], sum([c == None for c in row]))\n",
    ").collect()"
   ]
  },
  {
   "source": [
    "样本一共有6列，而id为3的样本就缺失了4列，查看id=3的样本，\n",
    "\n",
    "然后再根据每列的缺失情况（目的是看看还有没有填充解决的可能性），\n",
    "\n",
    "再决定是采用移除还是填充的方法。\n",
    "\n",
    "发现，id=3，缺失的是weight、age、gender和income。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+----+------+------+\n| id|weight|height| age|gender|income|\n+---+------+------+----+------+------+\n|  3|  null|   5.2|null|  null|  null|\n+---+------+------+----+------+------+\n\n"
    }
   ],
   "source": [
    "df_miss.where('id == 3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+----------+------------------+--------------+------------------+------------------+------------------+\n|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n+----------+------------------+--------------+------------------+------------------+------------------+\n|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n+----------+------------------+--------------+------------------+------------------+------------------+\n\n"
    }
   ],
   "source": [
    "# 统计每列缺失值情况\n",
    "df_miss.agg(*[\n",
    "    (1 - (fun.count(c) / fun.count('*'))).alias(c + '_missing')\n",
    "    for c in df_miss.columns]\n",
    ").show()"
   ]
  },
  {
   "source": [
    "1. income 列有72%是缺失的，远远大于50%，故要移除income列\n",
    "2. 发现weight、age和gender缺失情况不严重，可以通过填充来解决，id=3样本的缺失值。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------------------+------+---+-------+\n| id|            weight|height|age| gender|\n+---+------------------+------+---+-------+\n|  1|             143.5|   5.6| 28|      M|\n|  2|             167.2|   5.4| 45|      M|\n|  3|140.28333333333333|   5.2| 40|missing|\n|  4|             144.5|   5.9| 33|      M|\n|  5|             133.2|   5.7| 54|      F|\n|  6|             124.1|   5.2| 40|      F|\n|  7|             129.2|   5.3| 42|      M|\n+---+------------------+------+---+-------+\n\n"
    }
   ],
   "source": [
    "# 移除income列\n",
    "df_miss_no_income = df_miss.select([\n",
    "    c for c in df_miss.columns if c != 'income'\n",
    "])\n",
    "\n",
    "# 处理完列之后，如果要移除某些样本（某些行记录）的话，可以使用.dropna(thresh=3)的方法。\n",
    "# 即指定每一行样本缺失值的阈值，不超过阈值的行可以保留，否则移除。\n",
    "\n",
    "# 处理完列之后，如果决定要填充每行缺失值的话，可以使用.fillna(...)的方法\n",
    "\n",
    "# missing 填充 样本的 gender\n",
    "# 样本的其他项用均值填充\n",
    "\n",
    "# 计算除了gender列之外的数值型列的均值，数据类型转换为字典\n",
    "# 先是在Spark的DataFrame中计算每一列(除了gender)的均值，\n",
    "# 返回为数据类型是Spark的DataFrame，有列名和均值，gender只有列名\n",
    "# 再将返回值转换为Pandas的DataFrame数据类型\n",
    "# 最后将Pandas的DataFrame数据类型转换为字典\n",
    "means = df_miss_no_income.agg(\n",
    "    *[fun.mean(c).alias(c) \n",
    "      for c in df_miss_no_income.columns \n",
    "      if c != 'gender']\n",
    ").toPandas().to_dict('records')[0]    #-> toPandas()本质和.collect()一样，但是除非有成千上万个列，否则toPandas不会有问题。\n",
    "# 定义gender的缺失值\n",
    "means['gender'] = 'missing'\n",
    "# 用字典means来填充缺失值\n",
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'id': 4.0,\n 'weight': 140.28333333333333,\n 'height': 5.471428571428571,\n 'age': 40.4,\n 'gender': 'missing'}"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "source": [
    "注意：RDD中的.collect()和DataFrame中的.toPandas()只可以在数据量小的时候使用。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.3 检查离群值\n",
    "这里的检查离群值是针对非异常分析的情况，如果是异常分析，那么离群值才是最有价值的分析对象。\n",
    "\n",
    "这里的离群值指的是那些与样本其余部分的分布显著偏离的数据样本。显著性的定义各不相同，但是最普遍的形式是：\n",
    "\n",
    "如果数据值落在 Q1-1.5IQR 和 Q3 + 1.5IQR范围内，可以认为是没有离群值。否则认为是有离群值。\n",
    "\n",
    "其中， IQR定义为上分位和下分位之差，即75%（Q3）和25%(Q1)的差。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outliers = spark.createDataFrame([\n",
    "        (1, 143.5, 5.3, 28),\n",
    "        (2, 154.2, 5.5, 45),\n",
    "        (3, 342.3, 5.1, 99),\n",
    "        (4, 144.5, 5.5, 33),\n",
    "        (5, 133.2, 5.4, 54),\n",
    "        (6, 124.1, 5.1, 21),\n",
    "        (7, 129.2, 5.3, 42),\n",
    "    ], ['id', 'weight', 'height', 'age'])"
   ]
  },
  {
   "source": [
    "使用.approxQuantile(...)方法来实现上述离群值的定义，该方法一共有三个参数，\n",
    "\n",
    "第一个参数指定参数的列名，第二个参数指定0和1之间的一个数，或者是一个列表（本例子就是这样）\n",
    "\n",
    "第三参数指定每个度量可接受的错误程度，如果设为0，则表示只接受准确值，在这种估计计算中，这样不可取。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['weight', 'height', 'age']\n",
    "bounds = {}\n",
    "\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(\n",
    "        col, [0.25, 0.75], 0.05\n",
    "    )\n",
    "\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [\n",
    "        quantiles[0] - 1.5 * IQR,\n",
    "        quantiles[1] + 1.5 * IQR\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'weight': [91.69999999999999, 191.7],\n 'height': [4.499999999999999, 6.1000000000000005],\n 'age': [-11.0, 93.0]}"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+--------+-----+\n| id|weight_o|height_o|age_o|\n+---+--------+--------+-----+\n|  1|   false|   false|false|\n|  2|   false|   false|false|\n|  3|    true|   false| true|\n|  4|   false|   false|false|\n|  5|   false|   false|false|\n|  6|   false|   false|false|\n|  7|   false|   false|false|\n+---+--------+--------+-----+\n\n"
    }
   ],
   "source": [
    "# 使用bounds来标记离群值\n",
    "outliers = df_outliers.select(*['id'] + [\n",
    "    (\n",
    "        (df_outliers[c] < bounds[c][0]) |\n",
    "        (df_outliers[c] > bounds[c][1])\n",
    "    ).alias(c+'_o') for c in cols\n",
    "])\n",
    "\n",
    "outliers.show()"
   ]
  },
  {
   "source": [
    "id = 3的样本的weight和agg是离群值。除了过滤掉异常值之外，我们还可以查看这两个异常值。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+------+---+\n| id|weight|height|age|\n+---+------+------+---+\n|  7| 129.2|   5.3| 42|\n|  6| 124.1|   5.1| 21|\n|  5| 133.2|   5.4| 54|\n|  1| 143.5|   5.3| 28|\n|  2| 154.2|   5.5| 45|\n|  4| 144.5|   5.5| 33|\n+---+------+------+---+\n\n"
    }
   ],
   "source": [
    "# 过滤掉异常值:DataFrame的API查询方法\n",
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_no_outliers = df_outliers.select(\"id\", \"weight\", \"height\", \"age\").filter(\"weight_o = false\")\n",
    "df_no_outliers = df_no_outliers.select(\"id\", \"weight\", \"height\", \"age\").filter(\"height_o = false\")\n",
    "df_no_outliers = df_no_outliers.select(\"id\", \"weight\", \"height\", \"age\").filter(\"age_o = false\")\n",
    "df_no_outliers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+------+\n| id|weight|\n+---+------+\n|  3| 342.3|\n+---+------+\n\n+---+---+\n| id|age|\n+---+---+\n|  3| 99|\n+---+---+\n\n"
    }
   ],
   "source": [
    "# 查看异常数据\n",
    "df_outliers.filter(\"weight_o\").select(\"id\", \"weight\").show()\n",
    "df_outliers.filter(\"age_o\").select(\"id\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}