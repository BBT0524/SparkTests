{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. 第一个例子\n",
    "1. 读入文件为DataFrame\n",
    "2. 统计含有字母a的行数，含有字母b的行数，总共行数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Lines with a: 64, lines with b: 32, lines: 108\n"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 待读取文件路径\n",
    "logFile = \"D:\\\\spark\\\\README.md\" \n",
    "# 创建或者获取spark会话，并命名\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "# 读取文件到DataFrame\n",
    "logData = spark.read.text(logFile).cache()\n",
    "# 统计文件总行数\n",
    "nums = logData.count()\n",
    "# 统计含有字母a的行数\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "# 统计含有字母b的行数\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "# 打印结果\n",
    "print(\"Lines with a: %i, lines with b: %i, lines: %i\" % (numAs, numBs, nums))\n",
    "# 关闭spark会话\n",
    "spark.stop()"
   ]
  },
  {
   "source": [
    "## 2. 创建RDD\n",
    "也就是如何在。spark上创建一个数据集，这个是spark数据处理的基本对象.RDD是无schema的数据结构，和DataFrame非常不同。\n",
    "但是对于pysaprk来说，RDD需要在python和JVM之间来回切换，这样就增大了开销。而DataFrame没有这方面开销，相应地会更快一些。\n",
    "\n",
    "### 2.1 从变量或文件创建RDD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDTestApp\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# 方法1：.parallelize(...)集合（元素为list或array）\n",
    "data = sc.parallelize([('Amber',22), ('Alfred', 23), ('Skye', 4), ('Albert', 12), ('Amber', 9)])\n",
    "\n",
    "# 方法2：引用位于本地或者外部的某个文件\n",
    "data_from_file = sc.textFile('D:/spark/README.md', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10000"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# .parallelize(...)几乎可以混合使用任何类型的数据结构\n",
    "\n",
    "data_hetergenous = sc.parallelize([\n",
    "    ('Ferrari', 'fast'),\n",
    "    {'Porsche': 10000},\n",
    "    ['Spain', 'visited', 4504]\n",
    "]).collect()  # -> 对数据集使用.collect()方法，就就可以像python那样访问数据。\n",
    "\n",
    "data_hetergenous[1]['Porsche']"
   ]
  },
  {
   "source": [
    "### 2.2 读取创建的RDD对象\n",
    "如何读取创建的RDD对象。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['# Apache Spark']"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# 读取从文件创建的RDD对象data_from_file的第一行\n",
    "data_from_file.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['# Apache Spark', '']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "data_from_file.take(2)"
   ]
  },
  {
   "source": [
    "## 3 DataFrame\n",
    "DataFrame的引入极大地提升了python的性能。\n",
    "### 3.1 创建DataFrame\n",
    "一共两种方法：1，创建JSON数据，然后将其转换为DataFrame；2，从文件系统读入；\n",
    "#### 3.1.1 从自建的JSON文件中创建DataFrame\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# 创建spark会话\n",
    "spark = SparkSession.builder.appName(\"DataFrameTestApp\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'http://windows10.microdone.cn:4040'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 查看spark的UI web网址\n",
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第一步，创建自己的JSON数据\n",
    "stringJSONRDD = sc.parallelize((\"\"\"\n",
    "    {\n",
    "        \"id\": \"123\",\n",
    "        \"name\": \"Kate\",\n",
    "        \"age\": 19,\n",
    "        \"eyeColor\": \"brown\"\n",
    "    }\"\"\",\n",
    "    \"\"\"{\n",
    "        \"id\": \"234\",\n",
    "        \"name\": \"Michael\",\n",
    "        \"age\": 22,\n",
    "        \"eyeColor\": \"green\"\n",
    "    }\"\"\",\n",
    "    \"\"\"{\n",
    "        \"id\": \"345\",\n",
    "        \"name\": \"Simone\",\n",
    "        \"age\": 23,\n",
    "        \"eyeColor\": \"blue\"\n",
    "    }\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 第二步，从JSON创建一个DataFrame\n",
    "swimmersJSON = spark.read.json(stringJSONRDD)"
   ]
  },
  {
   "source": [
    "### 3.2 DataFram查询\n",
    "两种查询方式，一种是DataFrame自带的API，另一种是使用SQL查询语句；\n",
    "#### 3.2.1 DataFrame自带的API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|   Kate|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"
    }
   ],
   "source": [
    "## 简单的DataFrame查询\n",
    "swimmersJSON.show()"
   ]
  },
  {
   "source": [
    "#### 3.2.2 使用SQL语句查询"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[Row(age=19, eyeColor='brown', id='123', name='Kate'),\n Row(age=22, eyeColor='green', id='234', name='Michael'),\n Row(age=23, eyeColor='blue', id='345', name='Simone')]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# SQL查询语句\n",
    "## 需要先对DataFrame创建一个临时的视图\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmers\")\n",
    "## 对临时视图进行SQL语句查询\n",
    "spark.sql(\"select * from swimmers\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|   Kate|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"
    }
   ],
   "source": [
    "## 需要先对DataFrame创建一个临时的视图\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmers\")\n",
    "## 对SQL语句查询\n",
    "sqlDF = spark.sql(\"select * from swimmers\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+---+--------+---+-------+\n|age|eyeColor| id|   name|\n+---+--------+---+-------+\n| 19|   brown|123|   Kate|\n| 22|   green|234|Michael|\n| 23|    blue|345| Simone|\n+---+--------+---+-------+\n\n"
    }
   ],
   "source": [
    "## 需要先对DataFrame创建一个临时的视图\n",
    "swimmersJSON.createOrReplaceTempView(\"swimmers\")\n",
    "## 对SQL语句查询\n",
    "spark.sql(\"select * from swimmers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "source": [
    "### 3.3 例子统计文本中单词数量\n",
    "使用DataFrame数据集对象统计spark路径下的README.md文件中单词的数量；"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('#', 1),\n ('Apache', 1),\n ('Spark', 14),\n ('', 73),\n ('is', 7),\n ('a', 9),\n ('unified', 1),\n ('analytics', 1),\n ('engine', 2),\n ('for', 12),\n ('large-scale', 1),\n ('data', 2),\n ('processing.', 2),\n ('It', 2),\n ('provides', 1),\n ('high-level', 1),\n ('APIs', 1),\n ('in', 5),\n ('Scala,', 1),\n ('Java,', 1),\n ('Python,', 2),\n ('and', 9),\n ('R,', 1),\n ('an', 4),\n ('optimized', 1),\n ('that', 2),\n ('supports', 2),\n ('general', 2),\n ('computation', 1),\n ('graphs', 1),\n ('analysis.', 1),\n ('also', 5),\n ('rich', 1),\n ('set', 2),\n ('of', 5),\n ('higher-level', 1),\n ('tools', 1),\n ('including', 4),\n ('SQL', 2),\n ('DataFrames,', 1),\n ('MLlib', 1),\n ('machine', 1),\n ('learning,', 1),\n ('GraphX', 1),\n ('graph', 1),\n ('processing,', 1),\n ('Structured', 1),\n ('Streaming', 1),\n ('stream', 1),\n ('<https://spark.apache.org/>', 1),\n ('[![Jenkins', 1),\n ('Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n  1),\n ('[![AppVeyor', 1),\n ('Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n  1),\n ('[![PySpark', 1),\n ('Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n  1),\n ('##', 9),\n ('Online', 1),\n ('Documentation', 1),\n ('You', 3),\n ('can', 6),\n ('find', 1),\n ('the', 23),\n ('latest', 1),\n ('documentation,', 1),\n ('programming', 1),\n ('guide,', 1),\n ('on', 7),\n ('[project', 1),\n ('web', 1),\n ('page](https://spark.apache.org/documentation.html).', 1),\n ('This', 2),\n ('README', 1),\n ('file', 1),\n ('only', 1),\n ('contains', 1),\n ('basic', 1),\n ('setup', 1),\n ('instructions.', 1),\n ('Building', 1),\n ('built', 1),\n ('using', 3),\n ('[Apache', 1),\n ('Maven](https://maven.apache.org/).', 1),\n ('To', 2),\n ('build', 3),\n ('its', 1),\n ('example', 3),\n ('programs,', 1),\n ('run:', 1),\n ('./build/mvn', 1),\n ('-DskipTests', 1),\n ('clean', 1),\n ('package', 1),\n ('(You', 1),\n ('do', 2),\n ('not', 1),\n ('need', 1),\n ('to', 16),\n ('this', 1),\n ('if', 4),\n ('you', 4),\n ('downloaded', 1),\n ('pre-built', 1),\n ('package.)', 1),\n ('More', 1),\n ('detailed', 2),\n ('documentation', 3),\n ('available', 1),\n ('from', 1),\n ('project', 1),\n ('site,', 1),\n ('at', 2),\n ('[\"Building', 1),\n ('Spark\"](https://spark.apache.org/docs/latest/building-spark.html).', 1),\n ('For', 3),\n ('development', 1),\n ('tips,', 1),\n ('info', 1),\n ('developing', 1),\n ('IDE,', 1),\n ('see', 3),\n ('[\"Useful', 1),\n ('Developer', 1),\n ('Tools\"](https://spark.apache.org/developer-tools.html).', 1),\n ('Interactive', 2),\n ('Scala', 2),\n ('Shell', 2),\n ('The', 1),\n ('easiest', 1),\n ('way', 1),\n ('start', 1),\n ('through', 1),\n ('shell:', 2),\n ('./bin/spark-shell', 1),\n ('Try', 1),\n ('following', 2),\n ('command,', 2),\n ('which', 2),\n ('should', 2),\n ('return', 2),\n ('1,000,000,000:', 2),\n ('scala>', 1),\n ('spark.range(1000', 2),\n ('*', 4),\n ('1000', 2),\n ('1000).count()', 2),\n ('Python', 2),\n ('Alternatively,', 1),\n ('prefer', 1),\n ('use', 3),\n ('./bin/pyspark', 1),\n ('And', 1),\n ('run', 7),\n ('>>>', 1),\n ('Example', 1),\n ('Programs', 1),\n ('comes', 1),\n ('with', 3),\n ('several', 1),\n ('sample', 1),\n ('programs', 2),\n ('`examples`', 2),\n ('directory.', 1),\n ('one', 2),\n ('them,', 1),\n ('`./bin/run-example', 1),\n ('<class>', 1),\n ('[params]`.', 1),\n ('example:', 1),\n ('./bin/run-example', 2),\n ('SparkPi', 2),\n ('will', 1),\n ('Pi', 1),\n ('locally.', 1),\n ('MASTER', 1),\n ('environment', 1),\n ('variable', 1),\n ('when', 1),\n ('running', 1),\n ('examples', 2),\n ('submit', 1),\n ('cluster.', 1),\n ('be', 2),\n ('mesos://', 1),\n ('or', 3),\n ('spark://', 1),\n ('URL,', 1),\n ('\"yarn\"', 1),\n ('YARN,', 1),\n ('\"local\"', 1),\n ('locally', 2),\n ('thread,', 1),\n ('\"local[N]\"', 1),\n ('N', 1),\n ('threads.', 1),\n ('abbreviated', 1),\n ('class', 2),\n ('name', 1),\n ('package.', 1),\n ('instance:', 1),\n ('MASTER=spark://host:7077', 1),\n ('Many', 1),\n ('print', 1),\n ('usage', 1),\n ('help', 1),\n ('no', 1),\n ('params', 1),\n ('are', 1),\n ('given.', 1),\n ('Running', 1),\n ('Tests', 1),\n ('Testing', 1),\n ('first', 1),\n ('requires', 1),\n ('[building', 1),\n ('Spark](#building-spark).', 1),\n ('Once', 1),\n ('built,', 1),\n ('tests', 2),\n ('using:', 1),\n ('./dev/run-tests', 1),\n ('Please', 4),\n ('guidance', 2),\n ('how', 3),\n ('[run', 1),\n ('module,', 1),\n ('individual', 1),\n ('tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n  1),\n ('There', 1),\n ('Kubernetes', 1),\n ('integration', 1),\n ('test,', 1),\n ('resource-managers/kubernetes/integration-tests/README.md', 1),\n ('A', 1),\n ('Note', 1),\n ('About', 1),\n ('Hadoop', 3),\n ('Versions', 1),\n ('uses', 1),\n ('core', 1),\n ('library', 1),\n ('talk', 1),\n ('HDFS', 1),\n ('other', 1),\n ('Hadoop-supported', 1),\n ('storage', 1),\n ('systems.', 1),\n ('Because', 1),\n ('protocols', 1),\n ('have', 1),\n ('changed', 1),\n ('different', 1),\n ('versions', 1),\n ('Hadoop,', 2),\n ('must', 1),\n ('against', 1),\n ('same', 1),\n ('version', 1),\n ('your', 1),\n ('cluster', 1),\n ('runs.', 1),\n ('refer', 2),\n ('[\"Specifying', 1),\n ('Version', 1),\n ('Enabling', 1),\n ('YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n  1),\n ('building', 2),\n ('particular', 2),\n ('distribution', 1),\n ('Hive', 2),\n ('Thriftserver', 1),\n ('distributions.', 1),\n ('Configuration', 1),\n ('[Configuration', 1),\n ('Guide](https://spark.apache.org/docs/latest/configuration.html)', 1),\n ('online', 1),\n ('overview', 1),\n ('configure', 1),\n ('Spark.', 1),\n ('Contributing', 1),\n ('review', 1),\n ('[Contribution', 1),\n ('guide](https://spark.apache.org/contributing.html)', 1),\n ('information', 1),\n ('get', 1),\n ('started', 1),\n ('contributing', 1),\n ('project.', 1)]"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "# 建立或获取spark会话\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "# 将文件读到DataFrame数据集对象中，并将每一行原样转换到DataFrame中\n",
    "# 文件被认为是n行1列的格式，所以用row[0]代表文件的每一行\n",
    "lines = spark.read.text(\"d://spark//README.md\").rdd.map(lambda r: r[0])\n",
    "# 按照空格划分出单词\n",
    "# 将每个单词作为key，绑定到数字1上\n",
    "# 按照单词合并\n",
    "counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "                .map(lambda x: (x, 1)) \\\n",
    "                .reduceByKey(add)\n",
    "output = counts.collect()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}